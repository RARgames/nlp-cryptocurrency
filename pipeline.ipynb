{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('nlp': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ecf826560346b8e460920bdc8082639f49f33a7b7d65bafadd61cfcec99d7741"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input shape\ntrain:    (35653, 5)\nvalidate: (6292, 5)\ntest:     (7403, 5)\n\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import uuid\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "#####################################################\n",
    "# PARAMETERS\n",
    "#####################################################\n",
    "data_path = 'data/final_twitter.csv'\n",
    "results_path = 'data'\n",
    "embeddings_path = 'data/embeddings/glove.6B.100d.txt'\n",
    "embedding_dim = 100\n",
    "# sprawdzałem te inputy i raczej nie ma sytuacji żeby w 6h wrzucili ponad 200 tekstow\n",
    "window_width = 80 # number of rates to use (from window_with only values with time earlier than 6h are selected)\n",
    "txt_window = 15 # number of texts to use\n",
    "# series_window = 20\n",
    "epochs = 5\n",
    "early_stopping_min_delta = 1e-3\n",
    "early_stopping_patience = 2\n",
    "text_max_features = 30000 # number of most frequenct words to use\n",
    "text_sequence_length = 30 # number of word to take from single record\n",
    "categories = 3\n",
    "\n",
    "#####################################################\n",
    "\n",
    "hours_window = 6\n",
    "shuffle_buffer_size = 74726 # to fit whole training set in buffer\n",
    "\n",
    "df = pd.read_csv(data_path).drop('Unnamed: 0', axis=1)\n",
    "df.sort_values('date', axis=0, ascending=True, inplace=True, kind='quicksort', na_position='last', ignore_index=True, key=None)\n",
    "if categories == 2:\n",
    "    df['direction'] = df['direction'].map(lambda x : float(1 & int(x)))\n",
    "df = pd.concat([df, pd.get_dummies(df['direction'])], axis=1).drop(['direction'], axis=1)\n",
    "\n",
    "beg = datetime.datetime.strptime(df['date'][0], '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def date_to_hours(date: str):\n",
    "    d = datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "    return (d - beg).total_seconds() / 3600\n",
    "\n",
    "df['hours'] = df['date'].map(date_to_hours)\n",
    "df = df.drop(['date'], axis=1)\n",
    "\n",
    "shuffle = False\n",
    "train, test = train_test_split(df, shuffle=shuffle, test_size=0.15)\n",
    "train, validation = train_test_split(train, shuffle=shuffle, test_size=0.15)\n",
    "\n",
    "print('input shape\\ntrain:    {}\\nvalidate: {}\\ntest:     {}\\n'.format(train.shape, validation.shape, test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: 0.7025247469691914, 1: 1.6547207377135071, 2: 0.6121966348763278}"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "#all\n",
    "total = 74726\n",
    "c0 = 14985\n",
    "c1 = 17482\n",
    "c2 = 42259\n",
    "\n",
    "#twitter\n",
    "total = 31582\n",
    "c0 = 14985\n",
    "c1 = 6362\n",
    "c2 = 17196\n",
    "\n",
    "\n",
    "weight_for_0 = (1 / c0)*(total)/3.0 \n",
    "weight_for_1 = (1 / c1)*(total)/3.0\n",
    "weight_for_2 = (1 / c2)*(total)/3.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2}\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "  def __init__(self, input_width, txt_width, label_width=1, shift=1, text_max_features=1000, text_sequence_length=40,\n",
    "               train_df=None, val_df=None, test_df=None):\n",
    "    \n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    self.text_max_features = text_max_features\n",
    "    self.text_sequence_length =  text_sequence_length\n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.label_width = label_width\n",
    "    self.txt_width = txt_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.total_window_size = input_width + shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "    self.txt_input_start = self.total_window_size - self.txt_width\n",
    "    self.txt_input_slice = slice(self.txt_input_start, None)\n",
    "    self.txt_input_indices = np.arange(self.total_window_size)[self.txt_input_slice]\n",
    "\n",
    "    self.label_start = self.total_window_size - self.label_width\n",
    "    self.labels_slice = slice(self.label_start, None)\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    self._init_vectorizer()\n",
    "\n",
    "  def __repr__(self):\n",
    "    return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}'])\n",
    "\n",
    "  def _init_vectorizer(self):\n",
    "    self.vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "        max_tokens = self.text_max_features,\n",
    "        output_sequence_length = self.text_sequence_length\n",
    "    )\n",
    "\n",
    "    self.vectorizer.adapt(self.train_df['text'].values)\n",
    "\n",
    "\n",
    "  def split_window(self, features):\n",
    "    series_inputs = features[:, self.input_slice, -categories-1:-1]\n",
    "    series_times = features[:, self.input_slice, -1]\n",
    "\n",
    "    text_inputs = features[:, self.txt_input_slice, :-categories-1]\n",
    "    labels = features[:, self.labels_slice, -categories-1:-1]    \n",
    "    current_time = features[:, self.labels_slice, -1]    \n",
    "        \n",
    "\n",
    "    series_inputs.set_shape([None, self.input_width, None])\n",
    "    text_inputs.set_shape([None, self.txt_width, None])\n",
    "    labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "    series_times.set_shape([None, self.input_width])\n",
    "    current_time.set_shape([None, self.label_width])\n",
    "\n",
    "    # print('series_inputs', series_inputs.shape)\n",
    "    # print('series_times', series_times.shape)\n",
    "    # print('current_time', current_time.shape)\n",
    "    \n",
    "    mask = tf.math.less(series_times, current_time - hours_window)    \n",
    "    # print('mask', mask.shape)\n",
    "    series_inputs = tf.ragged.boolean_mask(series_inputs, mask)\n",
    "    # print('series_inputs masked', series_inputs.shape)\n",
    "    \n",
    "    \n",
    "    text_inputs = tf.reshape(text_inputs, [-1, self.txt_width*text_inputs.shape[2]])\n",
    "    labels = tf.reshape(labels, [-1, categories])\n",
    "  \n",
    "    return {'series': series_inputs, 'text': tf.cast(text_inputs, tf.int64)}, labels\n",
    "\n",
    "  def make_dataset(self, data, shuffle):\n",
    "    txt = np.array(self.vectorizer(data['text']).numpy(), dtype=np.float32)\n",
    "    data = np.array(data.drop('text', axis=1), dtype=np.float32)\n",
    "    data = np.concatenate([txt, data], axis=1)\n",
    "\n",
    "    ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "        data=data,\n",
    "        targets=None,\n",
    "        sequence_length=self.total_window_size,\n",
    "        sequence_stride=1,\n",
    "        shuffle=True,\n",
    "        batch_size=32,)\n",
    "\n",
    "    ds = ds.map(self.split_window)\n",
    "\n",
    "    return ds\n",
    "\n",
    "  def train(self):\n",
    "    return self.make_dataset(self.train_df, shuffle=True)\n",
    "\n",
    "  def val(self):\n",
    "    return self.make_dataset(self.val_df, shuffle=False)\n",
    "\n",
    "  def test(self):\n",
    "    return self.make_dataset(self.test_df, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dg = DataGenerator(input_width=window_width, txt_width=txt_window, label_width=1, shift=1, train_df=train, val_df=validation, test_df=test,\n",
    "#                    text_max_features=text_max_features, text_sequence_length=text_sequence_length)\n",
    "# tr = dg.train().take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_embeddings_matrix(vectorizer, embeddings_path, embedding_dim=100):\n",
    "    embeddings_index = {}\n",
    "    with open(embeddings_path) as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    voc = vectorizer.get_vocabulary()\n",
    "    word_index = dict(zip(voc, range(len(voc))))\n",
    "\n",
    "    num_tokens = len(voc) + 2\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "    # Prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n",
    "    return layers.Embedding(\n",
    "            num_tokens,\n",
    "            embedding_dim,\n",
    "            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "            trainable=False,\n",
    "            mask_zero=True\n",
    "    )\n",
    "\n",
    "def create_model_lstm(embedding_layer, window_length, num_labels=3):\n",
    "    \n",
    "    text_input = layers.Input(shape=(None,), name='text')\n",
    "    txt = embedding_layer(text_input)\n",
    "    txt = layers.Bidirectional(tf.keras.layers.LSTM(64, recurrent_dropout=0.5, dropout=0.5))(txt)\n",
    "\n",
    "    # series_input = layers.Input(shape=(window_length,num_labels), name='series')\n",
    "    # series = layers.LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(series_input)\n",
    "    # series = layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2)(series)    \n",
    "    # series = layers.Reshape([-1])(series)\n",
    "\n",
    "    # x = layers.concatenate([txt, series])\n",
    "\n",
    "    x = layers.Dropout(0.25)(txt)\n",
    "    out = layers.Dense(num_labels, activation='softmax')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=[text_input], outputs=[out])\n",
    "\n",
    "\n",
    "def create_model_gru(embedding_layer, window_length, num_labels=3):\n",
    "    \n",
    "    text_input = layers.Input(shape=(None,), name='text')\n",
    "    txt = embedding_layer(text_input)\n",
    "    txt = tf.keras.layers.GRU(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(txt)\n",
    "    txt = tf.keras.layers.GRU(64, dropout=0.2, recurrent_dropout=0.2)(txt)\n",
    "    # txt = layers.Bidirectional(tf.keras.layers.GRU(64, recurrent_dropout=0.5, dropout=0.5))(txt)\n",
    "\n",
    "    series_input = layers.Input(shape=(None, num_labels), name='series')\n",
    "    series = layers.GRU(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(series_input)\n",
    "    series = layers.GRU(64, dropout=0.2, recurrent_dropout=0.2)(series)    \n",
    "    series = layers.Reshape([-1])(series)\n",
    "\n",
    "    x = layers.concatenate([txt, series])\n",
    "\n",
    "    # txt = layers.Dropout(0.25)(x)\n",
    "\n",
    "    x = layers.Dense(64)(x)\n",
    "\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    out = layers.Dense(num_labels, activation='softmax')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=[text_input, series_input], outputs=[out])\n",
    "\n",
    "\n",
    "def create_model_lstm_big(embedding_layer, window_length, num_labels=3):\n",
    "    \n",
    "    text_input = layers.Input(shape=(None,), name='text')\n",
    "    txt = embedding_layer(text_input)\n",
    "    txt = layers.Bidirectional(tf.keras.layers.LSTM(64, recurrent_dropout=0.5, dropout=0.5))(txt)\n",
    "    txt = layers.Dense(32)(txt)\n",
    "\n",
    "    # series_input = layers.Input(shape=(window_length,num_labels), name='series')\n",
    "    # series = layers.LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(series_input)\n",
    "    # series = layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2)(series)\n",
    "    # series = layers.Dense(32)(series)\n",
    "    # series = layers.Reshape([-1])(series)\n",
    "\n",
    "    # x = layers.concatenate([txt, series])\n",
    "\n",
    "    x = layers.Dropout(0.25)(txt)\n",
    "    out = layers.Dense(num_labels, activation='softmax')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=[text_input], outputs=[out])\n",
    "\n",
    "\n",
    "def build_model(embeddings_layer, window_width, model_fn):\n",
    "    model = model_fn(embeddings_layer, window_width, categories)\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "    return model\n",
    "\n",
    "def early_stopping(min_delta=1e-3, patience=3):\n",
    "    return tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_categorical_accuracy',\n",
    "        min_delta=min_delta,\n",
    "        patience=patience,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "\n",
    "class Analyzer:\n",
    "\n",
    "    def __init__(self, reports_path):\n",
    "        self.root_reports_path = reports_path\n",
    "\n",
    "    def train_model(self, model, model_name, dg, model_parameters=None):\n",
    "\n",
    "        callbacks = callbacks=[early_stopping(early_stopping_min_delta, early_stopping_patience)]\n",
    "\n",
    "        history = model.fit(dg.train(), validation_data=dg.val(), batch_size=32, epochs=epochs, class_weight=class_weight, callbacks=callbacks)\n",
    "\n",
    "        self.model = model\n",
    "        \n",
    "        timestamp = datetime.datetime.now()\n",
    "\n",
    "        self.reports_path = self._report_directory(model_name, timestamp)\n",
    "\n",
    "        self._compute_metrics_val(dg)\n",
    "        self._compute_metrics_test(dg)\n",
    "\n",
    "        self._print_metrics(model_name)\n",
    "\n",
    "        self._generate_md_report(model_name, timestamp, model_parameters, history)\n",
    "        self._generate_metrics_json()\n",
    "\n",
    "        self._save_training_history(history)\n",
    "\n",
    "        model.save(os.path.join(self.reports_path, 'model'))\n",
    "\n",
    "\n",
    "    def _save_training_history(self, history):\n",
    "        pd.DataFrame(history.history).to_csv(os.path.join(self.reports_path, 'history.csv'), index_label='epoch')\n",
    "\n",
    "    def _compute_metrics_val(self, dg):\n",
    "        val_ds = dg.val()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for x, y in val_ds:    \n",
    "            y_pred.append(self.model.predict(x))\n",
    "            y_true.append(y.numpy())\n",
    "\n",
    "        yt = np.argmax(np.concatenate(y_true), axis=1)\n",
    "        yp = np.argmax(np.concatenate(y_pred), axis=1)\n",
    "        \n",
    "        self.accuracy_val = metrics.accuracy_score(yt, yp)\n",
    "        self.conf_matrix_val = metrics.confusion_matrix(yt, yp)\n",
    "\n",
    "    def _compute_metrics_test(self, dg):        \n",
    "        test_ds = dg.test()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for x, y in test_ds:    \n",
    "            y_pred.append(self.model.predict(x))\n",
    "            y_true.append(y.numpy())\n",
    "\n",
    "        yt = np.argmax(np.concatenate(y_true), axis=1)\n",
    "        yp = np.argmax(np.concatenate(y_pred), axis=1)\n",
    "\n",
    "        self.accuracy_test = metrics.accuracy_score(yt, yp)\n",
    "        self.conf_matrix_test = metrics.confusion_matrix(yt, yp)\n",
    "\n",
    "    def map_ds_to_array(self, ds):\n",
    "        y = []\n",
    "        for batch in ds:\n",
    "            y.append(batch)\n",
    "        return np.concatenate(y, axis=0)\n",
    "\n",
    "    def _print_metrics(self, model_name):\n",
    "        print('{} metrics:'.format(model_name))\n",
    "        print('accuracy val: {:.3f}'.format(self.accuracy_val))\n",
    "        print('accuracy test: {:.3f}'.format(self.accuracy_test))\n",
    "        print('confusion matrics val:\\n', self.conf_matrix_val)\n",
    "        print('confusion matrics test:\\n', self.conf_matrix_test)\n",
    "\n",
    "    def _generate_metrics_json(self):\n",
    "        m = {\n",
    "            'accuracy_val': self.accuracy_val,            \n",
    "            'conf_matrix_val': self.conf_matrix_val.tolist(),\n",
    "            'accuracy_test': self.accuracy_test,            \n",
    "            'conf_matrix_test': self.conf_matrix_test.tolist()\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(self.reports_path, 'metrics.json'), 'w') as f:\n",
    "            json.dump(m, f)\n",
    "\n",
    "    def _generate_md_report(self, model_name, timestamp, model_parameters, history):\n",
    "        with open(os.path.join(self.reports_path, 'report.md'), 'w') as f:\n",
    "            f.write('# {}\\n*{}*\\n'.format(model_name, timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "            self._add_model_summary(f)\n",
    "\n",
    "            if model_parameters is not None:\n",
    "                self._add_model_parameters(f, model_parameters)\n",
    "\n",
    "            self._add_metrics(f)\n",
    "            self._add_confusion_matrix(f, 'val', self.conf_matrix_val)\n",
    "            self._add_confusion_matrix(f, 'test', self.conf_matrix_test)\n",
    "            self._add_history(f, history)\n",
    "\n",
    "    def _add_confusion_matrix(self, file, mat_type, conf_mat):\n",
    "        text = '## Confusion matrix {}\\n'.format(mat_type)\n",
    "\n",
    "        class_num = conf_mat.shape[0]\n",
    "        text += ' | '.join([str(x) for x in range(1, class_num+1)]) + '\\n'\n",
    "        text += ' | '.join(['---']*class_num) + '\\n'\n",
    "        for row in conf_mat:\n",
    "            text += ' | '.join([str(x) for x in row])\n",
    "            text += '\\n'        \n",
    "        file.write(text)\n",
    "\n",
    "    def _add_metrics(self, file):\n",
    "        text = '## Metrics\\n'\n",
    "        text += '| Metric | Value \\n --- | ---\\n'\n",
    "        text += ' {} | {:.3f} \\n'.format('accuracy val', self.accuracy_val)\n",
    "        text += ' {} | {:.3f} \\n'.format('accuracy test', self.accuracy_test)\n",
    "        file.write(text)\n",
    "\n",
    "\n",
    "    def _add_model_parameters(self, file, parameters):\n",
    "        text = '### Model parameters\\n'\n",
    "        text += '| Prameters | Value \\n --- | ---\\n'\n",
    "        for k, v in parameters.items():\n",
    "            text += '{} | {}\\n'.format(k,v)\n",
    "        file.write(text)\n",
    "\n",
    "    def _add_model_summary(self, file):\n",
    "        file.write('## Model\\n```')\n",
    "        self.model.summary(print_fn=lambda x : file.write(x + '\\n'))\n",
    "        file.write('```\\n')\n",
    "\n",
    "    def _add_history(self, file, history):        \n",
    "        df = pd.DataFrame(history.history)\n",
    "        header = ' | '.join(df.columns)\n",
    "        text = '## History\\n'\n",
    "        text += header + '\\n'\n",
    "        text += ' | '.join(len(df.columns)*['---']) + '\\n'\n",
    "        for _, row in df.iterrows():\n",
    "            text += ' | '.join([str(round(x,4)) for x in row.values])\n",
    "            text += '\\n'\n",
    "        file.write(text)\n",
    "\n",
    "    def _report_directory(self, model_name, timestamp):\n",
    "        dir_name = 'results_{}_{}_{}/'.format(model_name, timestamp.strftime(\"%m-%d_%H-%M\"), uuid.uuid1().hex[:7])\n",
    "        path = os.path.join(self.root_reports_path, dir_name)\n",
    "        try:\n",
    "            os.mkdir(path)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "creating embeddings matrix\n",
      "Converted 15157 words (14843 misses)\n",
      "building model\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "series (InputLayer)             [(None, None, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, None, 100)    3000200     text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "gru_19 (GRU)                    (None, None, 64)     13248       series[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru_17 (GRU)                    (None, None, 64)     31872       embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru_20 (GRU)                    (None, 64)           24960       gru_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru_18 (GRU)                    (None, 64)           24960       gru_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 64)           0           gru_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128)          0           gru_18[0][0]                     \n",
      "                                                                 reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 64)           8256        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 3)            195         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,103,691\n",
      "Trainable params: 103,491\n",
      "Non-trainable params: 3,000,200\n",
      "__________________________________________________________________________________________________\n",
      "training and validating model\n",
      "Epoch 1/5\n",
      "1112/1112 [==============================] - 1509s 1s/step - loss: 0.8443 - categorical_accuracy: 0.4970 - val_loss: 0.9030 - val_categorical_accuracy: 0.5869\n",
      "Epoch 2/5\n",
      "1112/1112 [==============================] - 1358s 1s/step - loss: 0.8153 - categorical_accuracy: 0.5237 - val_loss: 0.8950 - val_categorical_accuracy: 0.6003\n",
      "Epoch 3/5\n",
      "1112/1112 [==============================] - 1299s 1s/step - loss: 0.8110 - categorical_accuracy: 0.5252 - val_loss: 0.8760 - val_categorical_accuracy: 0.6133\n",
      "Epoch 4/5\n",
      "1112/1112 [==============================] - 1496s 1s/step - loss: 0.8032 - categorical_accuracy: 0.5284 - val_loss: 0.8229 - val_categorical_accuracy: 0.6537\n",
      "Epoch 5/5\n",
      "1112/1112 [==============================] - 1964s 2s/step - loss: 0.7871 - categorical_accuracy: 0.5435 - val_loss: 0.8555 - val_categorical_accuracy: 0.6512\n",
      "gru metrics:\n",
      "accuracy val: 0.651\n",
      "accuracy test: 0.588\n",
      "confusion matrics val:\n",
      " [[  63  253  483]\n",
      " [ 103  394  699]\n",
      " [ 100  529 3588]]\n",
      "confusion matrics test:\n",
      " [[  66  321  817]\n",
      " [ 126  334  970]\n",
      " [ 153  627 3909]]\n",
      "INFO:tensorflow:Assets written to: data/results_gru_05-08_14-28_e31c471/model/assets\n",
      "INFO:tensorflow:Assets written to: data/results_gru_05-08_14-28_e31c471/model/assets\n"
     ]
    }
   ],
   "source": [
    "dg = DataGenerator(input_width=window_width, txt_width=txt_window, label_width=1, shift=1, train_df=train, val_df=validation, test_df=test, text_max_features=text_max_features, text_sequence_length=text_sequence_length)\n",
    "\n",
    "print(\"creating embeddings matrix\")\n",
    "emb_matrix = create_embeddings_matrix(dg.vectorizer, embeddings_path, embedding_dim)\n",
    "\n",
    "print(\"building model\")\n",
    "model = build_model(emb_matrix, window_width, create_model_gru)\n",
    "\n",
    "params = {\n",
    "    'window_width': window_width,\n",
    "    'embeddings_path': embeddings_path,\n",
    "    'embedding_dim': embedding_dim,\n",
    "    'epochs': epochs,\n",
    "    'early_stopping_min_delta': early_stopping_min_delta,\n",
    "    'early_stopping_patience': early_stopping_patience,\n",
    "    'text_max_features': text_max_features,\n",
    "    'text_sequence_length': text_sequence_length\n",
    "}\n",
    "\n",
    "an = Analyzer(results_path)\n",
    "print(\"training and validating model\")\n",
    "an.train_model(model, 'gru', dg, params)"
   ]
  }
 ]
}